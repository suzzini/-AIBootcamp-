#### 손실함수
- 손실 함수는 실제값과 예측값의 차이(loss, cost)를 수치화해주는 함수이다.
- 오차가 클수록 손실 함수의 값이 크고, 오차가 작을수록 손실 함수의 값이 작아진다.
- 손실 함수의 값을 최소화 하는 W, b를 찾아가는것이 학습 목표이다.


1. 회귀문제 : mean_squared_error(평균제곱오차)
    - 연속형 변수를 예측할 때 사용
  
2. 다중클래스분류 : categorical_crossentropy (범주형 교차 엔트로피)
    - 출력층의 활성화 함수명 : 소프트맥스
    - dens의 unit이 2 이상

3. 다중클래스분류 : sparse_categorical_crossentropy
    - 출력층의 활성화 함수명 : 소프트맥스
    - 범주형 교차 엔트로피와 동일하지만 이 경우 원-핫 인코딩이 된 상태일 필요없이 정수 인코딩 된 상태에서 수행 가능.

4. 이진분류 : binary_crossentropy(이항 교차 엔트로피)
    - 출력층의 활성화 함수명 : 시그모이드
    - 해당 손실함수 사용 시, dense 1이여야됨
  
 * * *
#### 이진분류의 경우, 

1) dense(1, activation='sigmoid'), loss='binary_crossentropy'

2) dense(2, activation='softmax'), loss='sparse_categorical_crossentropy' or 'categorical_crossentropy'

* * *

### 옵티마이저

> 신경망 학습의 목적 : 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것! => 최적화  

손실함수의 값을 줄여나가면서 학습하는 방법은 **어떤 옵티마이저**를 사용하느냐에 따라 달라짐(최적의 솔루션을 위해 최솟값을 찾기!)


1. 배치 경사 하강법(Batch Gradient Descent)
    - 배치 : 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양
    - 한 번의 epoch에 모든 매개변수 업데이트를 단 한 번 수행
    - 전체 데이터를 고려해서 학습하므로 epoch 당 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있으나 글로벌 미니멈을 찾을 수 있다는 장점이 있음
    - #model.fit(X_train, y_train, batch_size=len(trainX))
    
2. 확률적 경사 하강법(SGD)
    - 배치 경사 하강법의 속도 문제를 해결하기 위해 등장
    - 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법
    - 가장 크게 기울어진 방향으로 가자 ! 
    - 비등방성함수(기울기가 달라지는 함수)에서는 탐색 경로가 비효율적(지그재그로 탐색하는 경우)
3. 미니 배치 경사 하강법(Mini-Batch Gradient Descent)
    - 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법
    - 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이라는 장점
    - #model.fit(X_train, y_train, batch_size=32) #32를 배치 크기로 하였을 경우
4. 모멘텀(Momentum)
    - 관성이라는 물리학의 법칙을 응용한 방법
    - 관성의 힘을 빌리면 값이 조절되면서 로컬 미니멈에서 탈출하는 효과를 얻을 수 있음
5. 아다그라드(Adagrad)
    - 각 매개변수에 서로 다른 학습률을 적용(모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적)
    - 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정
    - 과거의 기울기를 제곱하여 계속 더해감
    - 학습을 계속 진행한 경우, 나중에 가서는 학습률이 지나치게 떨어진다는 단점(0이 됨)
        - 이를 개선한 옵티마이저 : RMSProp
 6. 아담(Adam)
    - Momentum + AdaGrad : 방향과 학습률 두 가지를 모두 잡기 위한 방법
    - 하이퍼파라미터의 '편향 보정'이 진행됨
