### 퍼셉트론
- 신경망을 이루는 가장 기본 단위
- 다수의 신호를 입력으로 받아 하나의 신호로 출력
- 데이터와 가중치들이 연산의 흐름을 만들어 정보를 계속해서 전달
- 각 뉴런(노드)의 연산값이 정해진 임계값(TLU)을 넘을 경우에만 다음 노드가 있는 layer로 신호를 전달
> 뉴런 :
> 들어온 입력값들을 단순히 가중치를 곱해 모두 더하는 연산(선형결합)을 하고 활성화함수(activation function)에 넣어 그 결과를 출력

#### 단층 퍼셉트론과 다층 퍼셉트론의 차이
단층 퍼셉트론은 입력층과 출력층만 존재하지만, 다층 퍼셉트론은 중간에 은닉층이 존재한다

=> 층을 여러겹으로 쌓아가면 선형 분류만으로 풀지 못했던 문제를 비선형적으로 풀 수 있게 됨 ! 


##### 활성함수
- 비선형성을 만들어 주는 구조(보낼지말지! => 이걸로 비선형성이 결정 됨
- 실제 뉴런세포에서 신호를 전달(activate or fire)할지 안 할지를 결정하는 기능
- ex) 계단함수, 시그모이드 함수, ReLU 함수
- 다음 층으로 신호를 얼마만큼 전달할지를 결정. 때때로 전달함수(transfer function)이라고 부르기도 함
* * *


### 신경망 
- 노드들이 가중치로 연결되어 입력값을 출력값으로 내보내는 함수
- 가중치를 찾는 과정을 **학습(training, learning)** 이라고 하며 가중치는 예측에 사용됨
- 입력층(input layers), 은닉층(hidden layers), 출력층(output layers) 

1. 입력층 : 어떠한 계산도 수행하지 않고 값을 전달, 신경망의 층수에 입력층은 포함x
2. 은닉층 : 입력층과 출력층 사이에 있는 층들, 2개 이상의 은닉층을 가진 신경망을 **딥러닝** 이라고 함
3. 출력층 : 출력층에는 대부분 활성함수가 존재(푸는 문제에 따라 다른 종류를 사용)
    - 회귀문제 : 예측할 목표 변수가 실수값인 경우 활성화 함수 필요x
    - 이진분류문제 : **시그모이드 함수** 사용
    - 다중클래스문제 : 출력층 노드가 부류 수 만큼 존재하며 **소프트맥스 함수** 를 활성화 함수로 사용(숫자분류)

* * *
### 용어정리
- epoch
> 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태
> (epoch 값이 너무 작다면 underfitting이 너무 크다면 overfitting이 발생할 확률이 높음)

- iteration
> The number of passes to complete one epoch

![image](https://user-images.githubusercontent.com/57658183/121131263-7af69100-c86a-11eb-98b2-5d9bbc22b902.png)
[참고링크](https://m.blog.naver.com/qbxlvnf11/221449297033)

* * *
### 역전파(Backpropagation)
신경망에 존재하는 가중치들을 어떻게 업데이트 해야할지 결정하기 위해 epoch/batch 마다 출력층에서 입력층 방향(역방향)으로 미분값을 계산하고 가중치를 업데이트 하여 신경망을 학습하는 알고리즘

0. 학습할 신경망 구조를 선택합니다.
    - 입력층 유닛의 수 = 특징 수
    - 출력층 유닛의 수 = 타겟 클래스 수
    - 은닉층 수, 각 은닉층의 노드 수
1. 가중치 랜덤 초기화
2. 순방향 전파를 통해   hθ(x(i)) (출력층 y값) 을 모든 입력 x(i)에 대해 계산합니다.
3. 비용함수를 J(θ)를 계산합니다.
4. 역방향 전파를 통해 편미분 값들 (δθ/δθljk)J(θ) 을 계산합니다.
5. 경사하강법 (or 다른 최적화 알고리즘)을 역전파와 함께 사용하여 비용함수인 J(θ) 를 최소화 합니다.
6. 어떤 중지 기준을 충족하거나 비용함수를 최소화 할 때까지 단계 2-5를 반복합니다. 2-5를 한 번 진행하는 것을 epoch 또는 iteration이라 말합니다.


### 경사하강법
경사(Gradient)를 계산해서 경사가 작아지도록 가중치를 업데이트!

* * *

### 모델 정의 : Sequential
1. 학습 데이터 로드
2. 모델 정의 : 입력층, 은닉층, 출력층을 정의하기 위해 keras에 Sequential()을 사용
    - model.add():층을 단계적으로 추가
    - model.add(Embedding(...)
    - **model.add(Dense(출력 뉴런의 수, input_dim = 입력 뉴런의 수, activation = 활성화 함수))**
3. 컴파일(Compile) : 모델을 기계가 이해할 수 있도록 컴파일
    - **model.compile(optimizer, loss, metrics)**
    - optimizer : 훈련 과정을 설정하는 옵티마이저를 설정('adam', 'sgd'와 같은 문자열로 지정)
    - loss : 훈련 과정에서 사용할 손실함수를 설정
    - metrics : 훈련을 모니터링하기 위한 지표 선택
4. 모델 학습(Fit) : 모델을 학습
    - **model.fit(훈련데이터, 레이블데이터, epoch, batch_size)**
    - epoch : 1은 전체 데이터를 한 차례 훑고 지나갔음을 의미함. 정수값 기재 필요. 총 훈련 횟수를 정의
    - batch_size : 기본값은 32. 미니 배치 경사 하강법을 사용하고 싶지 않을 경우에는 batch_size=None을 기재
    - verbose : 학습 중 출력되는 문구를 설정
    - validation_data(X_val,y_val)
5. 모델 검증(Evaluate) : 테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가
6. 모델 예측(Prediction) : 임의의 입력에 대한 모델의 출력값을 확인


* * * 
## 가중치 규제(Regularization)
<= overfitting을 해결할 수 있음

### EarlyStopping
학습을 좀만 시키기 !! 에러가 증가하는 부분에서 멈춰주기

### Weight Decay
- 가중치를 감소시키는 기술
- 손실함수(=objective function)에 weight의 크기를 나타내는 항을 추가하여 weight가 커지는 것을 방지(손실함수를 최소화하는 것이 목적이므로)
    - w가 커지면 overfitting이 될 수 잇으니 w의 크기까지 loss함수에 넣겠다!
- **L1 규제** : 가중치의 절댓값에 비례하는 비용이 추가
    - activity_regularizer=regularizers.l1(0.01)
- **L2 규제** : 가중치의 제곱에 비례하는 비용이 추가 (L1 규제는 일부 가중치 파라미터를 0으로 만듬. L2 규제는 가중치 파라미터를 제한하지만 완전히 0으로 만들지는 않아 L2 규제를 더 많이 사용함)
    - kernel_regularizer=regularizers.l2(0.01)

### Constraints
- 물리적으로 Weight의 크기를 제한
- kernel_constraint=MaxNorm(2.)) ## add constraints, 2보다 크지않게 제한

### Dropout 
- 신경망에서 가장 효과적이고 널리 사용하는 규제 기법 중 하나
- 훈련하는 동안 층의 출력 특성을 랜덤하게 끔(즉, 0으로 만듬
- 드롭아웃 비율 : 0이 되는 특성의 비율 (보통 0.2에서 0.5 사이를 사용)
- 바로 이전 층의 출력에 드롭아웃을 적용
    - keras.layers.Dense(16, activation='relu'),
    
      keras.layers.Dropout(0.5)
